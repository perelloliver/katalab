{
  "f55c2bb4-4689-4e4a-a862-cd83a8ab88cd": {
    "output_dir": "downloads/f55c2bb4-4689-4e4a-a862-cd83a8ab88cd",
    "company_info": {
      "roles": [
        {
          "title": "Senior Machine Learning Engineer (Platform & Inference)",
          "stack": [
            "PyTorch",
            "Docker",
            "Kubernetes",
            "gRPC",
            "Python",
            "KServe",
            "Seldon",
            "MLflow",
            "Weights & Biases",
            "CUDA"
          ],
          "requirements": "5+ years of software engineering experience, with at least 2 years in MLOps or MLE roles. Deep proficiency in Python (not just scripting, but building packages). Experience deploying ML models on Kubernetes (KServe, Seldon, or custom wrappers). Familiarity with MLflow, Weights & Biases, or similar experiment tracking tools. Understanding of distributed training (DDP, FSDP) is a huge plus.",
          "team": {
            "name": "Platform Team (Engineering)",
            "products": [
              {
                "name": "HelixServe",
                "type": "product",
                "description": "Internal model serving platform for wrapping complex PyTorch models (Transformers, Equivariant GNNs) into high-performance gRPC/HTTP APIs.",
                "resources": null
              },
              {
                "name": "Model-to-Metal Pipeline",
                "type": "product",
                "description": "Automated pipeline for the journey from a researcher's commit to a deployed model on our Kubernetes cluster, including automated quantization, TensorRT optimization, and regression testing.",
                "resources": null
              },
              {
                "name": "Reproducibility Engine",
                "type": "product",
                "description": "System that guarantees every prediction can be traced back to the exact code, data commit (DVC), and random seed used to generate it.",
                "resources": null
              }
            ],
            "clients": null,
            "size": 6,
            "context": "The Platform Team is focused on 'Stability at Scale', obsessed with uptime, and aims to impose order on the engineering process. They are responsible for turning research models into scalable, fault-tolerant inference services and automating the model deployment pipeline. They desperately try to impose order on the 'Two-Team' model friction.",
            "tools_used": [
              "Python 3.10+",
              "Poetry",
              "ruff",
              "Distroless Docker images",
              "Slurm",
              "Kubernetes (EKS)",
              "MLflow",
              "Weights & Biases",
              "Docker",
              "Terraform",
              "gRPC",
              "MyPy",
              "DVC"
            ],
            "philosophy": [
              "Stability at Scale",
              "Obsession with uptime",
              "Deterministic builds",
              "Hermetic environments",
              "CI/CD",
              "RFCs (Request for Comments)",
              "Blameless Post-Mortems",
              "Scout Rule (leave the codebase cleaner)",
              "Tough on code reviews (robustness, error handling, types, logging, efficiency)"
            ]
          }
        }
      ],
      "teams": [
        {
          "name": "Discovery Team (Research)",
          "products": null,
          "clients": null,
          "size": 12,
          "context": "The Discovery Team is responsible for creating state-of-the-art models and pushing scientific boundaries. They operate with a 'Publish or Perish (until it works)' mindset and often treat code as a throwaway artifact, believing engineering constraints can stifle creativity.",
          "tools_used": [
            "PyTorch",
            "Fortran",
            "PyTorch Lightning",
            "JupyterHub",
            "Local Laptop"
          ],
          "philosophy": [
            "Publish or Perish (until it works)",
            "Prioritize scientific output over engineering rigor",
            "Minimal process adherence",
            "Code as a 'throwaway artifact'"
          ]
        },
        {
          "name": "Platform Team (Engineering)",
          "products": [
            {
              "name": "HelixServe",
              "type": "product",
              "description": "Internal model serving platform for wrapping complex PyTorch models (Transformers, Equivariant GNNs) into high-performance gRPC/HTTP APIs.",
              "resources": null
            },
            {
              "name": "Model-to-Metal Pipeline",
              "type": "product",
              "description": "Automated pipeline for the journey from a researcher's commit to a deployed model on our Kubernetes cluster, including automated quantization, TensorRT optimization, and regression testing.",
              "resources": null
            },
            {
              "name": "Reproducibility Engine",
              "type": "product",
              "description": "System that guarantees every prediction can be traced back to the exact code, data commit (DVC), and random seed used to generate it.",
              "resources": null
            }
          ],
          "clients": null,
          "size": 6,
          "context": "The Platform Team is focused on 'Stability at Scale', obsessed with uptime, and aims to impose order on the engineering process. They are responsible for turning research models into scalable, fault-tolerant inference services and automating the model deployment pipeline. They desperately try to impose order on the 'Two-Team' model friction.",
          "tools_used": [
            "Python 3.10+",
            "Poetry",
            "ruff",
            "Distroless Docker images",
            "Slurm",
            "Kubernetes (EKS)",
            "MLflow",
            "Weights & Biases",
            "Docker",
            "Terraform",
            "gRPC",
            "MyPy",
            "DVC"
          ],
          "philosophy": [
            "Stability at Scale",
            "Obsession with uptime",
            "Deterministic builds",
            "Hermetic environments",
            "CI/CD",
            "RFCs (Request for Comments)",
            "Blameless Post-Mortems",
            "Scout Rule (leave the codebase cleaner)",
            "Tough on code reviews (robustness, error handling, types, logging, efficiency)"
          ]
        }
      ],
      "clients": [
        {
          "name": "Novartis",
          "type": "client",
          "description": "$50M strategic partnership to hunt for Alzheimer's targets using HelixFold.",
          "resources": null
        },
        {
          "name": "Roche",
          "type": "client",
          "description": "Multi-year collaboration to design novel antibody therapies for oncology.",
          "resources": null
        },
        {
          "name": "ETH Zurich & EPFL",
          "type": "client",
          "description": "Academic alliances where BioHelix funds 3 PhD positions annually and sponsors the 'BioHelix Chair for Computational Biology'.",
          "resources": null
        }
      ],
      "products": [
        {
          "name": "HelixFold\u2122 Engine",
          "type": "product",
          "description": "Our core technology; a proprietary geometric deep learning model that predicts protein-ligand binding affinity with 99.8% accuracy, surpassing AlphaFold 2 benchmarks. It enables de novo protein hallucination and side-chain packing optimization using custom PyTorch kernels and Equivariant GNNs.",
          "resources": null
        },
        {
          "name": "LabOS",
          "type": "product",
          "description": "The software that runs our robotic wet lab, automatically taking high-confidence candidates from HelixFold, instructing liquid handling robots to synthesize them, and piping assay results back into the training set for a 'Closed Loop' active learning cycle.",
          "resources": null
        },
        {
          "name": "BHX-001 (Lead Candidate for Cystic Fibrosis)",
          "type": "product",
          "description": "Our lead candidate for Cystic Fibrosis, with the objective to get IND-approved for Phase I clinical trials by FY 2026, requiring full data lineage and audit trails.",
          "resources": null
        }
      ],
      "philosophy": "Decoding Biology. Designing the Future. Our 10-year vision is to cure 50 rare genetic diseases by 2035. We believe that the future of medicine is programmable, aiming for the first end-to-end AI-designed biologic drug approved by the FDA by 2030, with zero human intervention in the design phase. At BioHelix, we are building a 'Science Factory' that requires standard operating procedures, strict tolerances, and safety checks."
    },
    "plan": {
      "title": "HelixFold Inference Service Kata: From Research Prototype to Production API",
      "description": "This kata challenges you to transform a research-grade PyTorch Equivariant Graph Neural Network (EGNN) model, inspired by our HelixFold\u2122 Engine, into a production-ready, high-performance gRPC inference service. You will navigate the challenges of bridging the gap between scientific discovery and engineering rigor, embodying the 'Stability at Scale' philosophy of our Platform Team. This involves not only implementing core functionality but also focusing on code quality, deterministic builds, and robust API design, mirroring the journey a model takes through our 'Model-to-Metal Pipeline'.",
      "tasks": [
        {
          "id": "model_refactoring_and_packaging",
          "name": "Refactoring and Python Packaging for Reproducibility",
          "description": "You are presented with a simplified PyTorch EGNN model (similar to those from our Discovery Team) that performs protein-ligand binding prediction. Your first task is to refactor this 'notebook-style' research code into a well-structured Python package. Focus on modularity, clear function signatures, comprehensive type hints (MyPy compatibility), and docstrings. Explain the 'why' behind your structural choices, particularly how this improves maintainability and serves as a foundation for the 'Reproducibility Engine' by ensuring deterministic dependencies and isolated environments.",
          "files": [
            "src/helixfold_inference/model.py",
            "src/helixfold_inference/data_types.py",
            "src/helixfold_inference/__init__.py",
            "pyproject.toml",
            "tests/test_model.py"
          ]
        },
        {
          "id": "code_quality_and_static_analysis",
          "name": "Enforcing Code Quality with Linting and Static Analysis",
          "description": "Building on the refactored code, your next step is to integrate and configure essential code quality tools: 'ruff' for linting and formatting, and 'MyPy' for static type checking. The Platform Team maintains a 'Tough on code reviews' philosophy, emphasizing robustness, error handling, and type correctness. Configure these tools to enforce strict standards, then apply them to your `helixfold_inference` package, fixing any reported issues. Document your configuration choices and explain how these tools contribute to 'Stability at Scale' and reduce technical debt in a fast-paced research environment.",
          "files": [
            ".ruff.toml",
            "mypy.ini",
            "src/helixfold_inference/model.py",
            "src/helixfold_inference/data_types.py"
          ]
        },
        {
          "id": "containerization_for_hermetic_environments",
          "name": "Containerization for Deterministic Deployment",
          "description": "To enable deployment through our 'Model-to-Metal Pipeline' and ensure 'Hermetic environments', you need to containerize your `helixfold_inference` package. Create a Dockerfile that builds a lean, production-ready image for your model. Emphasize using Python 3.10+ and consider best practices for minimizing image size (e.g., multi-stage builds, Distroless principles if applicable) and ensuring deterministic builds. Explain how Docker addresses environment inconsistencies that plague ML deployments and guarantees consistency across development, testing, and production stages.",
          "files": [
            "Dockerfile",
            "requirements.txt",
            ".dockerignore"
          ]
        },
        {
          "id": "grpc_inference_service_api",
          "name": "Building a gRPC Inference Service API with PyTorch",
          "description": "Now, wrap your containerized PyTorch EGNN model in a high-performance gRPC inference service, mirroring our 'HelixServe' platform. Define a `.proto` schema for your prediction API, handling input data (e.g., protein and ligand structures) and output predictions (binding affinity). Implement the gRPC server logic to load your model and perform inference, ensuring robust error handling, efficient data serialization/deserialization, and basic logging. Provide a simple gRPC client to demonstrate interaction with your service. Explain the advantages of gRPC over REST for high-throughput, low-latency ML inference within a microservices architecture.",
          "files": [
            "proto/helixfold_inference.proto",
            "src/helixserve/server.py",
            "src/helixserve/client.py",
            "src/helixserve/__init__.py"
          ]
        },
        {
          "id": "basic_artifact_and_reproducibility_tracking",
          "name": "Foundations of Artifact & Reproducibility Tracking",
          "description": "The 'Reproducibility Engine' is critical for our lead candidates like BHX-001, requiring full data lineage and audit trails. For this task, you will implement a foundational mechanism to track and log key artifacts and metadata related to your model's inference performance. Design a simple Python module or script that, when run, can: 1) Load the trained PyTorch model, 2) Perform inference on a small dataset, and 3) Log the model's version, key hyperparameters, inference metrics (e.g., average prediction time, a synthetic accuracy score), and the exact code commit ID (simulated). Discuss how this rudimentary system lays the groundwork for more advanced solutions like MLflow or Weights & Biases, which our Platform Team utilizes, to achieve deterministic traceability and aid in post-mortems.",
          "files": [
            "src/helixfold_tracker/tracker.py",
            "scripts/run_tracked_inference.py",
            "data/sample_inference_inputs.json",
            "config/inference_config.yaml"
          ]
        }
      ]
    }
  },
  "cdd0970b-1e0a-482a-884c-3ace7acd767d": {
    "output_dir": "downloads/cdd0970b-1e0a-482a-884c-3ace7acd767d",
    "company_info": {
      "roles": [
        {
          "title": "Senior Machine Learning Engineer (Platform & Inference)",
          "stack": [
            "Python",
            "PyTorch",
            "Docker",
            "Kubernetes",
            "gRPC",
            "MLflow",
            "TensorRT",
            "CUDA",
            "Terraform",
            "DVC"
          ],
          "requirements": "5+ years of software engineering experience, with at least 2 years in MLOps or MLE roles. Deep proficiency in Python (not just scripting, but building packages). Experience deploying ML models on Kubernetes (KServe, Seldon, or custom wrappers). Familiarity with MLflow, Weights & Biases, or similar experiment tracking tools. Understanding of distributed training (DDP, FSDP) is a huge plus. Must be a polyglot speaking 'Research' (PyTorch, Einsums, Gradient Checkpointing) and 'Production' (Docker, Kubernetes, Terraform, gRPC). Obsessed with deterministic builds, hermetic environments, and CI/CD. Able to profile slow models, spot bottlenecks (CPU, GPU memory bandwidth, Python GIL), and fix them (custom CUDA kernel, dataloader optimization).",
          "team": {
            "name": "Platform Team (Engineering)",
            "products": [
              {
                "name": "HelixServe",
                "type": "product",
                "description": "Internal model serving platform. Wraps complex PyTorch models (Transformers, Equivariant GNNs) into high-performance gRPC/HTTP APIs.",
                "resources": null
              },
              {
                "name": "The 'Model-to-Metal' Pipeline",
                "type": "product",
                "description": "Automates the journey from a researcher's commit to a deployed model on the Kubernetes cluster, including automated quantization, TensorRT optimization, and regression testing.",
                "resources": null
              },
              {
                "name": "Reproducibility Engine",
                "type": "product",
                "description": "System that guarantees every prediction can be traced back to the exact code, data commit (DVC), and random seed used to generate it.",
                "resources": null
              }
            ],
            "clients": null,
            "size": 6,
            "context": "This role is situated within the Platform Engineering Department, acting as a 'Missing Link' or 'DMZ' between the Discovery and Platform teams. The primary goal is to translate brilliant but 'messy' research code into scalable, fault-tolerant inference services and build guardrails for production. The team focuses on stability at scale, obsessing over uptime and imposing order, despite challenges with knowledge silos and cleaning up data from Research.",
            "tools_used": [
              "Python 3.10+",
              "Poetry",
              "ruff",
              "Pre-commit hooks",
              "Distroless Docker images",
              "Slurm",
              "Kubernetes (EKS)",
              "MyPy",
              "gRPC",
              "HTTP APIs",
              "TensorRT",
              "DVC",
              "Terraform"
            ],
            "philosophy": [
              "Stability at Scale",
              "Obsessed with uptime",
              "Impose order",
              "Science Factory mindset (standard operating procedures, strict tolerances, safety checks)",
              "Strict graduation process for code maturity",
              "Golden Path Stack",
              "RFCs (Request for Comments)",
              "Blameless Post-Mortems",
              "The Scout Rule (leave codebase cleaner)",
              "Tough on code reviews (robustness)",
              "Deterministic builds",
              "Hermetic environments",
              "CI/CD"
            ]
          }
        }
      ],
      "teams": [
        {
          "name": "Discovery Team (Research)",
          "products": null,
          "clients": null,
          "size": 12,
          "context": "Mission: 'Publish or Perish (until it works).' This team consists mostly of PhDs who create state-of-the-art models. They often treat code as a 'throwaway artifact' to generate figures, abandoning it after paper acceptance. The Head of Discovery believes engineering constraints stifle creativity, leading to friction with the Platform Team.",
          "tools_used": [
            "JupyterHub",
            "Local Laptop",
            "PyTorch",
            "Fortran",
            "PyTorch Lightning"
          ],
          "philosophy": [
            "Publish or Perish (until it works)",
            "Engineering constraints stifle creativity",
            "Code as a throwaway artifact"
          ]
        },
        {
          "name": "Platform Team (Engineering)",
          "products": [
            {
              "name": "HelixServe",
              "type": "product",
              "description": "Internal model serving platform. Wraps complex PyTorch models (Transformers, Equivariant GNNs) into high-performance gRPC/HTTP APIs.",
              "resources": null
            },
            {
              "name": "The 'Model-to-Metal' Pipeline",
              "type": "product",
              "description": "Automates the journey from a researcher's commit to a deployed model on the Kubernetes cluster, including automated quantization, TensorRT optimization, and regression testing.",
              "resources": null
            },
            {
              "name": "Reproducibility Engine",
              "type": "product",
              "description": "System that guarantees every prediction can be traced back to the exact code, data commit (DVC), and random seed used to generate it.",
              "resources": null
            }
          ],
          "clients": null,
          "size": 6,
          "context": "Mission: 'Stability at Scale.' This team is responsible for managing infrastructure, ensuring uptime, and imposing order on the codebase. They face challenges from the Discovery team's 'messy' code and critical knowledge silos within the team (e.g., Slurm/Kubernetes integration). They own the 'Golden Path' stack and enforce engineering standards.",
          "tools_used": [
            "Python 3.10+",
            "Poetry",
            "ruff",
            "Pre-commit hooks",
            "Distroless Docker images",
            "Slurm",
            "Kubernetes (EKS)",
            "MyPy",
            "gRPC",
            "HTTP APIs",
            "TensorRT",
            "DVC",
            "Terraform"
          ],
          "philosophy": [
            "Stability at Scale",
            "Obsessed with uptime",
            "Impose order",
            "Science Factory mindset (standard operating procedures, strict tolerances, safety checks)",
            "Strict graduation process for code maturity",
            "Golden Path Stack",
            "RFCs (Request for Comments)",
            "Blameless Post-Mortems",
            "The Scout Rule (leave codebase cleaner)",
            "Tough on code reviews (robustness)",
            "Deterministic builds",
            "Hermetic environments",
            "CI/CD"
          ]
        }
      ],
      "clients": [
        {
          "name": "Novartis",
          "type": "client",
          "description": "$50M strategic partnership to hunt for Alzheimer's targets using HelixFold.",
          "resources": null
        },
        {
          "name": "Roche",
          "type": "client",
          "description": "Multi-year collaboration to design novel antibody therapies for oncology.",
          "resources": null
        },
        {
          "name": "ETH Zurich & EPFL",
          "type": "client",
          "description": "Academic alliances where BioHelix funds 3 PhD positions annually and sponsors the 'BioHelix Chair for Computational Biology'.",
          "resources": null
        }
      ],
      "products": [
        {
          "name": "HelixFold\u2122 Engine",
          "type": "product",
          "description": "Our core technology. A proprietary geometric deep learning model that predicts protein-ligand binding affinity with 99.8% accuracy, surpassing AlphaFold 2 benchmarks on our internal 'hard targets' dataset. Capabilities include De novo protein hallucination, side-chain packing optimization.",
          "resources": null
        },
        {
          "name": "LabOS",
          "type": "product",
          "description": "The software that runs our robotic wet lab. Automatically takes high-confidence candidates from HelixFold, instructs liquid handling robots to synthesize them, and pipes assay results back into the training set, enabling a closed-loop 'Active Learning' cycle.",
          "resources": null
        }
      ],
      "philosophy": "Decoding Biology. Designing the Future. BioHelix is pioneering the field of Generative Biology with the vision to cure 50 rare genetic diseases by 2035. We believe that the future of medicine is programmable, aiming for the first end-to-end AI-designed biologic drug approved by the FDA by 2030, with zero human intervention in the design phase. We are building a Science Factory, which requires standard operating procedures, strict tolerances, and safety checks."
    },
    "plan": {
      "title": "HelixServe Model Onboarding Kata: From Research to Production",
      "description": "This kata simulates the process of taking a prototype machine learning model from the Discovery Team and preparing it for deployment on our internal HelixServe platform. You will refactor research-grade code for production readiness, containerize it using Docker, and set up a basic serving mechanism. This exercise is designed to be a guided tour of our engineering philosophy and tools, focusing on core concepts of stability, reproducibility, and the 'Model-to-Metal' pipeline.",
      "tasks": [
        {
          "id": "refactor_research_inference",
          "name": "Refactor a Research Inference Script",
          "description": "The Discovery Team has provided a basic PyTorch inference script for a novel protein-ligand binding prediction model. While functional, it's not production-ready. Your first task is to refactor this script to align with our production standards. This includes: 1) Extracting hardcoded paths/parameters into configurable command-line arguments. 2) Adding basic logging using Python's `logging` module. 3) Organizing the code into a more modular structure (e.g., separating model loading from inference logic). 4) Ensuring dependencies are managed via `uv` using a `pyproject.toml` file.",
          "files": [
            "src/model.py",
            "src/research_inference.py",
            "pyproject.toml"
          ]
        },
        {
          "id": "containerize_inference_service",
          "name": "Containerize the Refactored Inference Service",
          "description": "Now that the inference script is refactored, the next step is to containerize it using Docker. Create a `Dockerfile` that builds a minimal Docker image. The image should include all necessary dependencies (installed using `uv` based on the `pyproject.toml` from the previous task) and be able to execute the refactored inference script. Focus on creating an efficient, lean image suitable for our production environment, leveraging multi-stage builds if appropriate to minimize image size.",
          "files": [
            "Dockerfile",
            "src/model.py",
            "src/production_inference.py",
            "pyproject.toml"
          ]
        },
        {
          "id": "expose_inference_endpoint",
          "name": "Expose Model Inference via a Simple HTTP API",
          "description": "To make the model consumable by HelixServe and other internal services, we need to expose its inference capabilities via an API. For this task, you'll extend the containerized service to expose a simple HTTP POST endpoint using Flask (since it's in your known stack). This endpoint should accept a basic input (e.g., a dummy protein/ligand ID or simplified payload) and return a placeholder prediction result. This simulates the initial step of integrating a model into our 'Model-to-Metal' pipeline, where it would eventually become part of HelixServe's gRPC/HTTP API.",
          "files": [
            "Dockerfile",
            "src/model.py",
            "src/production_inference.py",
            "src/app.py",
            "pyproject.toml"
          ]
        }
      ]
    }
  }
}